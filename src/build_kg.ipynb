{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import collections\n",
    "# import pdfplumber\n",
    "# import os\n",
    "# import json\n",
    "\n",
    "# from classBook import Book\n",
    "\n",
    "\n",
    "# dataDir = \"../data/\"\n",
    "# dataName = \"Deep Learning.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import pdfplumber\n",
    "\n",
    "dataDir = \"../data/\"\n",
    "dataName = \"Deep Learning.pdf\"\n",
    "\n",
    "\n",
    "class Book:\n",
    "    num_pages = -1\n",
    "    book_end_page = \"735\"\n",
    "    page_offset = 15\n",
    "\n",
    "    def __init__(self, name=dataName, dataDir=dataDir):\n",
    "        self.name = name\n",
    "        self.dataDir = dataDir\n",
    "        self.pdf = self.loader()\n",
    "\n",
    "    def loader(self):\n",
    "        print(\"Reading book: \", self.name, \"from directory: \", self.dataDir)\n",
    "        try:\n",
    "            pdf = pdfplumber.open(os.path.join(self.dataDir, self.name))\n",
    "        except:\n",
    "            print(\"Error: File not found\")\n",
    "            return None\n",
    "        self.num_pages = len(pdf.pages)\n",
    "        print(\"Book loaded successfully\")\n",
    "        print(\"Number of pages: \", self.num_pages)\n",
    "        return pdf\n",
    "\n",
    "    def close(self):\n",
    "        self.pdf.close()\n",
    "\n",
    "    def readPage(self, page=-1):\n",
    "        if page == -1:\n",
    "            return self.pdf.pages\n",
    "        else:\n",
    "            return self.pdf.pages[page - 1]\n",
    "\n",
    "    def readPageInInterval(self, start_page, end_page, offset=page_offset):\n",
    "        return self.pdf.pages[start_page - 1 + offset : end_page - 1 + offset]\n",
    "\n",
    "    def searchStrInPage(self, page, str):\n",
    "        page_text = self.readPage(page)\n",
    "        return page_text.extract_text().lower().find(str.lower())\n",
    "\n",
    "    def getToc(self):\n",
    "        pages = []\n",
    "        for i in range(8):\n",
    "            if self.searchStrInPage(i, \"Contents\") != -1:\n",
    "                pages.append(i)\n",
    "        # reg expression to match '6 Deep Feedforward Networks 168'\n",
    "        pattern_chapter = re.compile(r\"(\\d+)\\s+(.*)\\s+(\\d+)\")\n",
    "        # match '6.1 Example: Learning XOR . . . . . . . . . . . . . . . . . . . . . . . 171',\n",
    "        pattern_section = re.compile(\n",
    "            r\"(\\d+)\\.(\\d+)\\s+([\\?\\,\\'\\’\\(\\)a-zA-Z\\:\\s\\-]+)\\s+.*\\s+(\\d+)\"\n",
    "        )\n",
    "\n",
    "        # save to dict\n",
    "        toc = {}\n",
    "        for page in pages:\n",
    "            page_text = self.readPage(page)\n",
    "            text = page_text.extract_text()\n",
    "            lines = text.split(\"\\n\")\n",
    "            for line in lines:\n",
    "                match_chapter = pattern_chapter.match(line)\n",
    "                match_section = pattern_section.match(line)\n",
    "                if match_chapter:\n",
    "                    chapter = {\n",
    "                        \"chapter\": match_chapter.group(1),\n",
    "                        \"title\": match_chapter.group(2),\n",
    "                        \"page\": match_chapter.group(3),\n",
    "                    }\n",
    "                elif match_section:\n",
    "                    section = {\n",
    "                        \"chapter\": match_section.group(1),\n",
    "                        \"section\": match_section.group(2),\n",
    "                        \"title\": match_section.group(3),\n",
    "                        \"page\": match_section.group(4),\n",
    "                    }\n",
    "                    if chapter[\"chapter\"] not in toc:\n",
    "                        toc[chapter[\"chapter\"]] = {\n",
    "                            \"title\": chapter[\"title\"],\n",
    "                            \"page\": chapter[\"page\"],\n",
    "                            \"sections\": [],\n",
    "                        }\n",
    "                    toc[chapter[\"chapter\"]][\"sections\"].append(section)\n",
    "\n",
    "        # add end page\n",
    "        for chapter in toc:\n",
    "            try:\n",
    "                toc[chapter][\"end_page\"] = toc[str(int(chapter) + 1)][\"page\"]\n",
    "            except:\n",
    "                toc[chapter][\"end_page\"] = self.book_end_page\n",
    "            for section in toc[chapter][\"sections\"]:\n",
    "                try:\n",
    "                    section[\"end_page\"] = toc[chapter][\"sections\"][\n",
    "                        int(section[\"section\"])\n",
    "                    ][\"page\"]\n",
    "                except:\n",
    "                    section[\"end_page\"] = toc[chapter][\"end_page\"]\n",
    "        # write to json\n",
    "        with open(os.path.join(self.dataDir, \"toc.json\"), \"w\") as f:\n",
    "            json.dump(toc, f, indent=4)\n",
    "        return len(toc)\n",
    "\n",
    "    def loadToc(self):\n",
    "        with open(os.path.join(self.dataDir, \"toc.json\"), \"r\") as f:\n",
    "            toc = json.load(f)\n",
    "        return toc\n",
    "\n",
    "    def getChapter(self, chapter):\n",
    "        toc = self.loadToc()\n",
    "        page = toc[chapter][\"page\"]\n",
    "        end_page = toc[chapter][\"end_page\"]\n",
    "        return self.readPageInInterval(int(page), int(end_page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading book:  Deep Learning.pdf from directory:  ../data/\n",
      "Book loaded successfully\n",
      "Number of pages:  800\n"
     ]
    }
   ],
   "source": [
    "book = Book(dataName)\n",
    "# book.getToc()\n",
    "# toc = book.loadToc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\.conda\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import math\n",
    "import torch\n",
    "import wikipedia\n",
    "import IPython\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations_from_model_output(text):\n",
    "    relations = []\n",
    "    relation, subject, relation, object_ = \"\", \"\", \"\", \"\"\n",
    "    text = text.strip()\n",
    "    current = \"x\"\n",
    "    text_replaced = text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
    "    for token in text_replaced.split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = \"t\"\n",
    "            if relation != \"\":\n",
    "                relations.append(\n",
    "                    {\n",
    "                        \"head\": subject.strip(),\n",
    "                        \"type\": relation.strip(),\n",
    "                        \"tail\": object_.strip(),\n",
    "                    }\n",
    "                )\n",
    "                relation = \"\"\n",
    "            subject = \"\"\n",
    "        elif token == \"<subj>\":\n",
    "            current = \"s\"\n",
    "            if relation != \"\":\n",
    "                relations.append(\n",
    "                    {\n",
    "                        \"head\": subject.strip(),\n",
    "                        \"type\": relation.strip(),\n",
    "                        \"tail\": object_.strip(),\n",
    "                    }\n",
    "                )\n",
    "            object_ = \"\"\n",
    "        elif token == \"<obj>\":\n",
    "            current = \"o\"\n",
    "            relation = \"\"\n",
    "        else:\n",
    "            if current == \"t\":\n",
    "                subject += \" \" + token\n",
    "            elif current == \"s\":\n",
    "                object_ += \" \" + token\n",
    "            elif current == \"o\":\n",
    "                relation += \" \" + token\n",
    "    if subject != \"\" and relation != \"\" and object_ != \"\":\n",
    "        relations.append(\n",
    "            {\"head\": subject.strip(), \"type\": relation.strip(), \"tail\": object_.strip()}\n",
    "        )\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KB:\n",
    "    def __init__(self):\n",
    "        self.relations = []\n",
    "\n",
    "    def are_relations_equal(self, r1, r2):\n",
    "        return all(r1[attr] == r2[attr] for attr in [\"head\", \"type\", \"tail\"])\n",
    "\n",
    "    def exists_relation(self, r1):\n",
    "        return any(self.are_relations_equal(r1, r2) for r2 in self.relations)\n",
    "\n",
    "    def add_relation(self, r):\n",
    "        if not self.exists_relation(r):\n",
    "            self.relations.append(r)\n",
    "\n",
    "    def print(self):\n",
    "        print(\"Relations:\")\n",
    "        for r in self.relations:\n",
    "            print(f\"  {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_small_text_to_kb(text, verbose=False):\n",
    "    kb = KB()\n",
    "\n",
    "    # Tokenizer text\n",
    "    model_inputs = tokenizer(\n",
    "        text, max_length=512, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    if verbose:\n",
    "        print(f\"Num tokens: {len(model_inputs['input_ids'][0])}\")\n",
    "\n",
    "    # Generate\n",
    "    gen_kwargs = {\n",
    "        \"max_length\": 216,\n",
    "        \"length_penalty\": 0,\n",
    "        \"num_beams\": 3,\n",
    "        \"num_return_sequences\": 3,\n",
    "    }\n",
    "    generated_tokens = model.generate(\n",
    "        **model_inputs,\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "\n",
    "    # create kb\n",
    "    for sentence_pred in decoded_preds:\n",
    "        relations = extract_relations_from_model_output(sentence_pred)\n",
    "        for r in relations:\n",
    "            kb.add_relation(r)\n",
    "\n",
    "    return kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 512\n",
      "Relations:\n",
      "  {'head': 'intelligent software', 'type': 'use', 'tail': 'artificial intelligence'}\n",
      "  {'head': 'Galatea', 'type': 'instance of', 'tail': 'legendary figure'}\n",
      "  {'head': 'Pygmalion', 'type': 'instance of', 'tail': 'legendary figure'}\n",
      "  {'head': 'Deep Blue', 'type': 'instance of', 'tail': 'AI'}\n",
      "  {'head': 'Deep Blue', 'type': 'uses', 'tail': 'AI deep learning'}\n",
      "  {'head': 'Deep Blue', 'type': 'use', 'tail': 'AI deep learning'}\n",
      "  {'head': 'algorithm', 'type': 'part of', 'tail': 'computer science'}\n",
      "  {'head': 'computer science', 'type': 'has part', 'tail': 'artificial intelligence'}\n",
      "  {'head': 'artificial intelligence', 'type': 'part of', 'tail': 'computer science'}\n",
      "  {'head': 'algorithm', 'type': 'part of', 'tail': 'artificial intelligence'}\n",
      "  {'head': 'artificial intelligence', 'type': 'has part', 'tail': 'algorithm'}\n",
      "  {'head': 'au- toencoder', 'type': 'instance of', 'tail': 'algorithm'}\n",
      "  {'head': 'au- toencoder', 'type': 'subclass of', 'tail': 'Autoencoder'}\n",
      "  {'head': 'au- toencoder', 'type': 'subclass of', 'tail': 'Autoencoders'}\n",
      "  {'head': 'Deep learning', 'type': 'instance of', 'tail': 'abstractions'}\n",
      "  {'head': 'Deep learning', 'type': 'subclass of', 'tail': 'abstractions'}\n",
      "  {'head': 'Hidden layer', 'type': 'subclass of', 'tail': 'layers'}\n",
      "  {'head': 'hidden layer', 'type': 'subclass of', 'tail': 'input data'}\n",
      "  {'head': 'hidden layer', 'type': 'has part', 'tail': 'input'}\n",
      "  {'head': 'input', 'type': 'part of', 'tail': 'hidden layer'}\n",
      "  {'head': 'σ(wTx)', 'type': 'instance of', 'tail': 'logistic regression'}\n",
      "  {'head': 'σ(wTx)', 'type': 'facet of', 'tail': 'logistic regression'}\n",
      "  {'head': 'computational graph', 'type': 'subclass of', 'tail': 'flow chart'}\n",
      "  {'head': 'Deep learning', 'type': 'subclass of', 'tail': 'machine learning'}\n",
      "  {'head': 'deep learning', 'type': 'subclass of', 'tail': 'machine learning'}\n",
      "  {'head': 'Deep learning', 'type': 'subclass of', 'tail': 'artificial intelligence'}\n",
      "  {'head': 'representation learning', 'type': 'subclass of', 'tail': 'machine learning'}\n",
      "  {'head': 'AI', 'type': 'has part', 'tail': 'machine systems learning'}\n",
      "  {'head': 'machine systems learning', 'type': 'part of', 'tail': 'AI'}\n",
      "  {'head': 'Deep Classic Rule', 'type': 'subclass of', 'tail': 'learning machine systems'}\n",
      "  {'head': 'Deep learning', 'type': 'subclass of', 'tail': 'algorithms'}\n",
      "  {'head': 'CNN', 'type': 'subclass of', 'tail': 'Deep Networks'}\n",
      "  {'head': 'Deep Networks', 'type': 'subclass of', 'tail': 'RNNs'}\n",
      "  {'head': 'CNN', 'type': 'part of', 'tail': 'Deep Networks'}\n",
      "  {'head': 'cybernetics', 'type': 'subclass of', 'tail': 'deep learning'}\n",
      "  {'head': 'Deep learning', 'type': 'subclass of', 'tail': 'artificial neural network'}\n",
      "  {'head': 'Deep learning', 'type': 'instance of', 'tail': 'artificial neural network'}\n",
      "  {'head': 'neural network', 'type': 'has part', 'tail': 'hidden layer'}\n",
      "  {'head': 'hidden layer', 'type': 'part of', 'tail': 'neural network'}\n",
      "  {'head': 'neural network', 'type': 'has part', 'tail': 'hidden layers'}\n",
      "  {'head': 'hidden layers', 'type': 'part of', 'tail': 'neural network'}\n",
      "  {'head': '1940', 'type': 'point in time', 'tail': '1940'}\n",
      "  {'head': '1960', 'type': 'point in time', 'tail': '1960'}\n",
      "  {'head': '1990', 'type': 'point in time', 'tail': '1990'}\n",
      "  {'head': 'McCulloch-Pitts Neuron', 'type': 'instance of', 'tail': 'linear model'}\n",
      "  {'head': 'McCulloch-Pitts Neuron', 'type': 'instance of', 'tail': 'algorithm'}\n",
      "  {'head': 'stochastic gradient descent', 'type': 'instance of', 'tail': 'algorithm'}\n",
      "  {'head': 'rectified linear unit', 'type': 'subclass of', 'tail': 'neural network'}\n",
      "  {'head': 'rectified linear unit', 'type': 'subclass of', 'tail': 'neuron'}\n",
      "  {'head': 'rectified linear unit', 'type': 'instance of', 'tail': 'neural network'}\n",
      "  {'head': 'Connectionism', 'type': 'discoverer or inventor', 'tail': 'Donald Hebb'}\n",
      "  {'head': 'Connectionism', 'type': 'named after', 'tail': 'Donald Hebb'}\n",
      "  {'head': 'Cognition', 'type': 'studied by', 'tail': 'cognition'}\n",
      "  {'head': 'Geoffrey Hinton', 'type': 'employer', 'tail': 'University of Toronto'}\n",
      "  {'head': 'Yann LeCun', 'type': 'employer', 'tail': 'University of Montreal'}\n",
      "  {'head': 'Yann LeCun', 'type': 'employer', 'tail': 'University of Toronto'}\n",
      "  {'head': 'Deep learning', 'type': 'discoverer or inventor', 'tail': 'Geoffrey Hinton'}\n",
      "  {'head': 'Deep learning', 'type': 'subclass of', 'tail': 'supervised learning'}\n",
      "  {'head': 'deep belief network', 'type': 'use', 'tail': 'deep learning'}\n",
      "  {'head': 'connections', 'type': 'part of', 'tail': 'neuron'}\n",
      "  {'head': 'unsupervised', 'type': 'subclass of', 'tail': 'machine learning'}\n",
      "  {'head': 'unsupervised', 'type': 'part of', 'tail': 'machine learning'}\n",
      "  {'head': '1900', 'type': 'point in time', 'tail': '1900'}\n",
      "  {'head': '2000s', 'type': 'followed by', 'tail': '2010s'}\n",
      "  {'head': '2010s', 'type': 'follows', 'tail': '2000s'}\n",
      "  {'head': 'MNIST dataset', 'type': 'named after', 'tail': 'National Institute of Standards and Technology'}\n",
      "  {'head': 'Geoffrey Hinton', 'type': 'field of work', 'tail': 'machine learning'}\n",
      "  {'head': 'MNIST dataset', 'type': 'publisher', 'tail': 'National Institute of Standards and Technology'}\n",
      "  {'head': 'deep learning', 'type': 'uses', 'tail': 'GPUs'}\n",
      "  {'head': 'Deep learning', 'type': 'uses', 'tail': 'convolutional network'}\n",
      "  {'head': 'ImageNet Large Scale Visual Recognition Challenge', 'type': 'facet of', 'tail': 'deep learning'}\n",
      "  {'head': 'Biological neural network sizes from Wikipedia (2015)', 'type': 'facet of', 'tail': 'artificial neural network'}\n",
      "  {'head': 'Biological neural network sizes from Wikipedia (2015)', 'type': 'facet of', 'tail': 'artificial neural networks'}\n",
      "  {'head': 'Biological neural network sizes from Wikipedia (2015)', 'type': 'is a list of', 'tail': 'artificial neural network'}\n",
      "  {'head': 'DeepLearning', 'type': 'use', 'tail': 'reinforcement learning'}\n",
      "  {'head': 'Deep learning', 'type': 'use', 'tail': 'reinforcement learning'}\n",
      "  {'head': 'DeepMind', 'type': 'product or material produced', 'tail': 'reinforcement learning'}\n",
      "  {'head': 'Deep learning', 'type': 'instance of', 'tail': 'machine learning'}\n",
      "  {'head': 'Deepbelief network', 'type': 'instance of', 'tail': 'neural network'}\n",
      "  {'head': 'Deepbelief network', 'type': 'subclass of', 'tail': 'neural network'}\n",
      "  {'head': 'Deepbelief network', 'type': 'instance of', 'tail': 'artificial neural network'}\n",
      "  {'head': '2010', 'type': 'point in time', 'tail': '2010'}\n",
      "  {'head': 'ImageNet Large Scale Visual Recognition Challenge', 'type': 'point in time', 'tail': '2015'}\n",
      "  {'head': '2012', 'type': 'point in time', 'tail': '2012'}\n",
      "  {'head': 'Applied Math', 'type': 'has part', 'tail': 'Machine Learning'}\n",
      "  {'head': 'Applied Math and Machine Learning Basics', 'type': 'number of episodes', 'tail': '29'}\n",
      "  {'head': 'Applied Math and Machine Learning Basics 29', 'type': 'point in time', 'tail': '29'}\n",
      "  {'head': 'training algorithm', 'type': 'subclass of', 'tail': 'algorithm'}\n",
      "  {'head': 'training algorithm', 'type': 'facet of', 'tail': 'machine learning'}\n",
      "  {'head': 'training algorithm', 'type': 'part of', 'tail': 'machine learning'}\n"
     ]
    }
   ],
   "source": [
    "test = [page.extract_text().replace(\"\\n\", \" \") for page in book.getChapter(\"1\")]\n",
    "# save to txt\n",
    "# with open(os.path.join(dataDir, \"test.txt\"), \"w\") as f:\n",
    "#     f.write(test)\n",
    "\n",
    "\n",
    "kb = from_small_text_to_kb(test, verbose=True)\n",
    "kb.print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
